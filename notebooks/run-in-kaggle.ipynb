{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, QuantileTransformer\n",
    "from sklearn import decomposition, cluster\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.weight_norm import WeightNorm\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import multiprocessing\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_submission = False\n",
    "is_kaggle = True\n",
    "is_train_without_holdout = True\n",
    "\n",
    "if is_kaggle:\n",
    "    import sys\n",
    "    sys.path.append('../input/iterative-stratification/iterative-stratification-master') \n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    path_data = '../input/lish-moa/'\n",
    "    path_output = ''\n",
    "else:\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    path_data = 'data/'\n",
    "    path_output = 'data/'\n",
    "\n",
    "if is_submission:\n",
    "    validation_ratio = 0.0\n",
    "    test_features = pd.read_csv(path_data+'test_features.csv')\n",
    "    run_script = test_features.shape[0] > 3982\n",
    "    if run_script==False:\n",
    "        !cp ../input/lish-moa/sample_submission.csv .\n",
    "        !mv ./sample_submission.csv ./submission.csv\n",
    "else:\n",
    "    validation_ratio = 0.2\n",
    "    run_script = True\n",
    "    \n",
    "if is_train_without_holdout:\n",
    "    validation_ratio = 0.0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "calculate_new_prepared_data = False\n",
    "dump_prepared_data = False\n",
    "prepared_data_version = 'p21'\n",
    "\n",
    "train_model = True\n",
    "if train_model:\n",
    "    path_model = ''\n",
    "else:\n",
    "    if is_kaggle:\n",
    "        path_model = '../input/'+'modelp21lb/'\n",
    "    else:\n",
    "        path_model = ''\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "SHIFT = 2222\n",
    "\n",
    "n_seeds = 5\n",
    "\n",
    "cpu = multiprocessing.cpu_count()\n",
    "\n",
    "folds = 5\n",
    "fold_type = None # 'multi' but without drugs separation\n",
    "\n",
    "oof_type = 'multi' #can be 'kfold' or 'multi'\n",
    "oof_threshold = 18 \n",
    "\n",
    "add_kernels = False\n",
    "use_log_for_kernel_diff = True\n",
    "inverse_kde = False\n",
    "ratio_inverse_kde = True\n",
    "use_diff_kde = False #When set to true, use_log_for_kernel_diff, inverse_kde, ratio_inverse_kde become irrelevant\n",
    "exclude_c_from_kde = False\n",
    "exclude_g_from_kde = True\n",
    "\n",
    "g_removal_count = 75 #the higher the more columns to drop\n",
    "add_c_stats = False\n",
    "\n",
    "\n",
    "perform_pca = False\n",
    "pca_for_c = False\n",
    "pca_for_kde = False\n",
    "\n",
    "use_train_test_for_norm = True\n",
    "normalization_type = 'standard' #standard or quantile\n",
    "\n",
    "control_share_in_train = 0.0\n",
    "add_control_from_test = False\n",
    "\n",
    "augment_data = False\n",
    "additional = 0 #won't calculate if zero\n",
    "granularity = 100\n",
    "max_dev = 0.1\n",
    "normal_std_dev = 0.1\n",
    "\n",
    "label_smoothing = 0.00025\n",
    "    \n",
    "model_nonscored = False\n",
    "\n",
    "oof_loss_limit = 0.0\n",
    "\n",
    "nn_architecture = 'v1.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusivity_tuples = [\n",
    " ['potassium_channel_activator',\n",
    "  'potassium_channel_antagonist',\n",
    "  'potassium_channel_agonist',\n",
    "  'potassium_channel_blocker'],\n",
    " ['atp-sensitive_potassium_channel_antagonist',\n",
    "  'atp-sensitive_potassium_channel_agonist',\n",
    "  'atp-sensitive_potassium_channel_inhibitor'],\n",
    " ['gaba_receptor_agonist',\n",
    "  'gaba_receptor_modulator'],\n",
    " ['glutamate_receptor_agonist',\n",
    "  'glutamate_receptor_antagonist',\n",
    "  'glutamate_receptor_modulator'],\n",
    " ['nitric_oxide_donor',\n",
    "  'nitric_oxide_scavenger',\n",
    "  'nitric_oxide_stimulant'],\n",
    " ['prostanoid_receptor_antagonist',\n",
    "  'prostanoid_receptor_agonist',\n",
    " ' prostanoid_receptor_inhibitor'],\n",
    " ['sodium_channel_inhibitor',\n",
    "  'sodium_channel_activator',\n",
    "  'sodium_channel_blocker'],\n",
    " ['acetylcholine_receptor_agonist',\n",
    "  'acetylcholine_receptor_antagonist'],\n",
    " ['adenosine_receptor_agonist',\n",
    "  'adenosine_receptor_antagonist'],\n",
    " ['adenylyl_cyclase_activator',\n",
    "  'adenylyl_cyclase_inhibitor'],\n",
    " ['adrenergic_receptor_agonist',\n",
    "  'adrenergic_receptor_antagonist'],\n",
    " ['aldehyde_dehydrogenase_inhibitor',\n",
    "  'aldehyde_dehydrogenase_activator'],\n",
    " ['ampk_activator',\n",
    "  'ampk_inhibitor'],\n",
    " ['androgen_receptor_agonist',\n",
    "  'androgen_receptor_antagonist'],\n",
    " ['angiotensin_receptor_antagonist',\n",
    "  'angiotensin_receptor_agonist'],\n",
    " ['apoptosis_stimulant', \n",
    "  'apoptosis_inhibitor'],\n",
    " ['aryl_hydrocarbon_receptor_agonist', \n",
    "  'aryl_hydrocarbon_receptor_antagonist'],\n",
    " ['atp_channel_activator', \n",
    "  'atp_channel_blocker'],\n",
    " ['benzodiazepine_receptor_agonist', \n",
    "  'benzodiazepine_receptor_antagonist'],\n",
    " ['calcium_channel_blocker',\n",
    "  'calcium_channel_activator'],\n",
    " ['cannabinoid_receptor_agonist', \n",
    "  'cannabinoid_receptor_antagonist'],\n",
    " ['car_agonist', \n",
    "  'car_antagonist'],\n",
    " ['caspase_activator', \n",
    "  'caspase_inhibitor'],\n",
    " ['cc_chemokine_receptor_antagonist',\n",
    "  'cc_chemokine_receptor_agonist'],\n",
    " ['cftr_channel_agonist', \n",
    "  'cftr_channel_antagonist'],\n",
    " ['chloride_channel_blocker',\n",
    "  'chloride_channel_activator'],\n",
    " ['cholinergic_receptor_antagonist',\n",
    "  'cholinergic_receptor_agonist'],\n",
    " ['complement_antagonist', \n",
    "  'complement_inhibitor'],\n",
    " ['corticosteroid_agonist',\n",
    "  'corticosteroid_antagonist'],\n",
    " ['dopamine_receptor_agonist',\n",
    "  'dopamine_receptor_antagonist'],\n",
    " ['estrogen_receptor_agonist',\n",
    "  'estrogen_receptor_antagonist'],\n",
    " ['fatty_acid_receptor_agonist',\n",
    "  'fatty_acid_receptor_antagonist'],\n",
    " ['fxr_agonist', \n",
    "  'fxr_antagonist'],\n",
    " ['g_protein-coupled_receptor_agonist',\n",
    "  'g_protein-coupled_receptor_antagonist'],\n",
    " ['glucocorticoid_receptor_agonist',\n",
    "  'glucocorticoid_receptor_antagonist'],\n",
    " ['glucokinase_activator',\n",
    "  'glucokinase_inhibitor'],\n",
    " ['gonadotropin_receptor_agonist',\n",
    "  'gonadotropin_receptor_antagonist'],\n",
    " ['guanylate_cyclase_activator',\n",
    "  'guanylate_cyclase_stimulant'],\n",
    " ['histamine_receptor_agonist',\n",
    "  'histamine_receptor_antagonist'],\n",
    " ['hsp_inhibitor',\n",
    "  'hsp_inducer'],\n",
    " ['icam1_antagonist',\n",
    "  'icam1_inhibitor'],\n",
    " ['membrane_permeability_enhancer',\n",
    "  'membrane_permeability_inhibitor'],\n",
    " ['mineralocorticoid_receptor_antagonist',\n",
    "  'mineralocorticoid_receptor_agonist'],\n",
    " ['neurotensin_receptor_agonist',\n",
    "  'neurotensin_receptor_antagonist'],\n",
    " ['nfkb_inhibitor', \n",
    "  'nfkb_activator'],\n",
    " ['opioid_receptor_agonist',\n",
    "  'opioid_receptor_antagonist'],\n",
    " ['oxytocin_receptor_agonist',\n",
    "  'oxytocin_receptor_antagonist'],\n",
    " ['p53_activator',\n",
    "  'p53_inhibitor'],\n",
    " ['phospholipase_inhibitor',\n",
    "  'phospholipase_activator'],\n",
    " ['pka_activator',\n",
    "  'pka_inhibitor'],\n",
    " ['ppar_receptor_agonist',\n",
    "  'ppar_receptor_antagonist'],\n",
    " ['progesterone_receptor_agonist',\n",
    "  'progesterone_receptor_antagonist'],\n",
    " ['protein_kinase_inhibitor', \n",
    "  'protein_kinase_activator'],\n",
    " ['protein_synthesis_inhibitor',\n",
    "  'protein_synthesis_stimulant'],\n",
    " ['retinoid_receptor_agonist', \n",
    "  'retinoid_receptor_antagonist'],\n",
    " ['serotonin_receptor_agonist', \n",
    "  'serotonin_receptor_antagonist'],\n",
    " ['sigma_receptor_agonist', \n",
    "  'sigma_receptor_antagonist'],\n",
    " ['sirt_activator',\n",
    "  'sirt_inhibitor'],\n",
    " ['smoothened_receptor_antagonist',\n",
    "  'smoothened_receptor_agonist'],\n",
    " ['src_inhibitor',\n",
    "  'src_activator'],\n",
    " ['thyroid_hormone_inhibitor',\n",
    "  'thyroid_hormone_stimulant'],\n",
    " ['tlr_agonist',\n",
    "  'tlr_antagonist'],\n",
    " ['trace_amine_associated_receptor_agonist',\n",
    "  'trace_amine_associated_receptor_antagonist'],\n",
    " ['transient_receptor_potential_channel_antagonist',\n",
    "  'transient_receptor_potential_channel_agonist'],\n",
    " ['trpv_agonist',\n",
    "  'trpv_antagonist'],\n",
    " ['urotensin_receptor_agonist',\n",
    "  'urotensin_receptor_antagonist'],\n",
    " ['vasopressin_receptor_agonist',\n",
    "  'vasopressin_receptor_antagonist'],\n",
    " ['wnt_inhibitor',\n",
    "  'wnt_agonist']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cffd96223435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacOSFile(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        # print(\"reading total_bytes=%s\" % n, flush=True)\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                # print(\"reading bytes [%s,%s)...\" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                # print(\"done.\", flush=True)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def pickle_load(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prepareData:\n",
    "    def __init__(self,path,validation_ratio=0,folds=5,\n",
    "                 g_removal_count=0, add_c_stats=False, normalization_type='standard',\n",
    "                 add_kernels=False, use_log_for_kernel_diff=False, inverse_kde=False, ratio_inverse_kde=False, use_diff_kde=False,exclude_c_from_kde=False,exclude_g_from_kde=False,\n",
    "                 perform_pca=False, pca_variance_threshold=0.95, pca_for_kde=False, pca_for_c=True,\n",
    "                 use_train_test_for_norm=True,cpu=None,\n",
    "                 granularity=100,max_dev=0.1,normal_std_dev=0.1,additional=10):\n",
    "        self.path = path\n",
    "        self.folds = 5\n",
    "        self.use_log_for_kernel_diff = use_log_for_kernel_diff\n",
    "        self.normalization_type = normalization_type\n",
    "        self.add_kernels = add_kernels\n",
    "        self.add_c_stats = add_c_stats\n",
    "        self.perform_pca = perform_pca\n",
    "        self.pca_variance_threshold = pca_variance_threshold\n",
    "        self.use_log_for_kernel_diff = use_log_for_kernel_diff\n",
    "        self.inverse_kde = inverse_kde\n",
    "        self.use_diff_kde = use_diff_kde\n",
    "        self.exclude_c_from_kde = exclude_c_from_kde\n",
    "        self.exclude_g_from_kde = exclude_g_from_kde\n",
    "        \n",
    "        \n",
    "        if cpu is None:\n",
    "            cpu = multiprocessing.cpu_count()\n",
    "            self.cpu = cpu\n",
    "        else:\n",
    "            cpu = min(cpu,multiprocessing.cpu_count())\n",
    "            self.cpu = cpu\n",
    "            \n",
    "        print('import data')\n",
    "        self._import_data(self.path)\n",
    "        self._num_features = list(set(self.X_train.columns) - set(['sig_id','cp_type','cp_dose','cp_time']))\n",
    "        \n",
    "        print('transform cat features')\n",
    "        self.X_train = self._transform_cat_features(self.X_train)\n",
    "        self.X_test = self._transform_cat_features(self.X_test)\n",
    "        \n",
    "        print('kde kernels calculations')\n",
    "        self.kde_kernels = self._calculate_kde_kernels(self.X_train,self.X_test,ratio_inverse_kde)\n",
    "        \n",
    "        print('remove g columns with low variation')\n",
    "        self.g_to_drop = self._calculate_g_cols_to_drop(self._num_features,self.kde_kernels,g_removal_count)\n",
    "        self.X_train.drop(self.g_to_drop,inplace=True,axis=1)\n",
    "        self.X_test.drop(self.g_to_drop,inplace=True,axis=1)\n",
    "        self._num_features = list(set(self.X_train.columns) - set(['sig_id','cp_type','cp_dose','cp_time']))\n",
    "        \n",
    "        if add_kernels:\n",
    "            print('kde features')\n",
    "            self.X_train = self._process_kde_parallelized(self.X_train,self.kde_kernels,use_log_for_kernel_diff,inverse_kde,use_diff_kde,cpu,exclude_c_from_kde,exclude_g_from_kde)\n",
    "            self.X_test = self._process_kde_parallelized(self.X_test,self.kde_kernels,use_log_for_kernel_diff,inverse_kde,use_diff_kde,cpu,exclude_c_from_kde,exclude_g_from_kde)\n",
    "            \n",
    "        if add_c_stats:\n",
    "            print('add survavilability stats (c)')\n",
    "            self.X_train = self._add_c_stats(self.X_train)\n",
    "            self.X_test = self._add_c_stats(self.X_test)\n",
    "            \n",
    "        if perform_pca:\n",
    "            print('perform pca')\n",
    "            self._fit_pca([self.X_train,self.X_test],pca_for_kde,pca_for_c)\n",
    "            self.X_train = self._transform_pca(self.X_train,pca_variance_threshold)\n",
    "            self.X_test = self._transform_pca(self.X_test,pca_variance_threshold)\n",
    "        \n",
    "        print('normalize features')\n",
    "        if use_train_test_for_norm:\n",
    "            _ = self._normalize_features(pd.concat([self.X_train,self.X_test],axis=0))\n",
    "            self.X_train = self._normalize_features(self.X_train,is_test=True)\n",
    "            self.X_test = self._normalize_features(self.X_test,is_test=True)\n",
    "        else:\n",
    "            self.X_train = self._normalize_features(self.X_train)\n",
    "            self.X_test = self._normalize_features(self.X_test,is_test=True)\n",
    "            \n",
    "        \n",
    "        if additional>0:\n",
    "            print('data augmentation')\n",
    "            X_list = [self.X_train,self.X_test]\n",
    "\n",
    "            prob_dist = self._calculate_prob_dist_for_data_augmentation(X_list,granularity)\n",
    "            self.var_list = self._generate_data_augmentation(self.X_train,prob_dist,granularity,max_dev=max_dev,normal_std_dev=normal_std_dev,additional=additional)\n",
    "        \n",
    "    \n",
    "    def prepare_test_data(self,path):\n",
    "        X_test = pd.read_csv(self.path+'test_features.csv')\n",
    "        X_test = self._transform_cat_features(X_test)\n",
    "        X_test.drop(self.g_to_drop,inplace=True,axis=1)\n",
    "        if self.add_kernels:\n",
    "            X_test = self._process_kde_parallelized(X_test,self.kde_kernels,self.use_log_for_kernel_diff,self.inverse_kde,self.use_diff_kde,self.cpu,self.exclude_c_from_kde,self.exclude_g_from_kde)\n",
    "        if self.add_c_stats:\n",
    "            X_test = self._add_c_stats(X_test)\n",
    "        if self.perform_pca:\n",
    "            X_test = self._transform_pca(X_test,self.pca_variance_threshold)\n",
    "        X_test = self._normalize_features(X_test,is_test=True)\n",
    "        \n",
    "        self.X_test = X_test\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _index_to_boolean(self,idx,size):\n",
    "        mask_array = np.zeros(size)\n",
    "        mask_array = mask_array==1\n",
    "        mask_array[idx] = True\n",
    "        return mask_array\n",
    "    def _boolean_to_index(self,boolean_array):\n",
    "        idx = np.arange(0,boolean_array.shape[0])\n",
    "        idx = idx[boolean_array].copy()\n",
    "        return idx\n",
    "        \n",
    "    def _import_data(self,path):\n",
    "        self.X_train = pd.read_csv(path+'train_features.csv')\n",
    "        self.X_test = pd.read_csv(path+'test_features.csv')\n",
    "        self.y_train = pd.read_csv(path+'train_targets_scored.csv')\n",
    "        self.X_train_additional = pd.read_csv(path+'train_targets_nonscored.csv')\n",
    "        self.X_train_drugs = pd.read_csv(path+'train_drug.csv')\n",
    "        self.sample_submission = pd.read_csv(path+'sample_submission.csv')\n",
    "        \n",
    "        \n",
    "    def _transform_cat_features(self,X):\n",
    "        X['cp_type'] = X['cp_type'].map({'trt_cp':0,'ctl_vehicle':1})\n",
    "        X['cp_dose'] = X['cp_dose'].map({'D1':0,'D2':1})\n",
    "        X['cp_time'] = X['cp_time'].map({24:0,48:0.5,72:1})\n",
    "        return X\n",
    "    \n",
    "    def _normalize_features(self,X,is_test=False):\n",
    "        cols_to_normalize = list(set(self.X_train.columns) - set(['sig_id','cp_type','cp_dose','cp_time']))\n",
    "        if is_test==False:\n",
    "            self.normalizer_dict = {}\n",
    "        for col in cols_to_normalize:\n",
    "            if is_test:\n",
    "                scaler = self.normalizer_dict[col]\n",
    "                a = X[col].values.reshape(-1,1)\n",
    "                a = (scaler.transform(a)).flatten()\n",
    "                \n",
    "                if (self.normalization_type == 'quantile') and (not '_kde_diff' in col):\n",
    "                    a = a/10.0 + 0.5\n",
    "                X[col] = a\n",
    "            else:\n",
    "                if (self.normalization_type == 'quantile') and (not '_kde_diff' in col):\n",
    "                    scaler = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "                else:\n",
    "                    scaler = MinMaxScaler()\n",
    "                \n",
    "                a = X[col].values\n",
    "                a = scaler.fit_transform(a.reshape(-1, 1))\n",
    "                self.normalizer_dict[col] = scaler\n",
    "                \n",
    "                if (self.normalization_type == 'quantile') and (not '_kde_diff' in col):\n",
    "                    #Quantilization transforms data on a [-5:+5] range here\n",
    "                    a = a/10.0 + 0.5\n",
    "                    \n",
    "                X[col] = a\n",
    "                \n",
    "        return X\n",
    "    \n",
    "    def _calculate_kde_kernels(self,X1,X2,ratio_inverse_kde):\n",
    "        X = pd.concat([X1,X2])\n",
    "        X_control = X[X['cp_type']==1]\n",
    "        X_treatment = X[X['cp_type']==0]\n",
    "        kernels = {}\n",
    "        cols = self._num_features\n",
    "        for col in cols:\n",
    "            #Calculate kernels\n",
    "            x_control = X_control[col].values\n",
    "            x_treatment = X_treatment[col].values\n",
    "            kde_control_kernel = stats.gaussian_kde(x_control)\n",
    "            kde_treatment_kernel = stats.gaussian_kde(x_treatment)\n",
    "            kernels[col+'_control'] = kde_control_kernel\n",
    "            kernels[col+'_treatment'] = kde_treatment_kernel\n",
    "            \n",
    "            #Calculate max ratio so that when calculating kde features based on the ratio of treatement/control, we have a threshold for values\n",
    "            x_control_mean = x_control.mean()\n",
    "            x_control_std = x_control.std()\n",
    "            x_treatment_mean = x_treatment.mean()\n",
    "            #As b is not usually normal we use only a std to create range\n",
    "            kde_range = [min(x_control_mean - 2*x_control_std, x_treatment_mean - 2*x_control_std),max(x_control_mean + 2*x_control_std, x_treatment_mean + 2*x_control_std)]\n",
    "            kde_sample = np.arange(kde_range[0],kde_range[1],(kde_range[1]-kde_range[0])/100)\n",
    "            \n",
    "            x_control_kde_sample = kde_control_kernel.pdf(kde_sample)\n",
    "            x_treatment_kde_sample = kde_treatment_kernel.pdf(kde_sample)\n",
    "            if ratio_inverse_kde:\n",
    "                max_ratio = (x_control_kde_sample/x_treatment_kde_sample).max()\n",
    "            else:\n",
    "                max_ratio = (x_treatment_kde_sample/x_control_kde_sample).max()\n",
    "            kernels[col+'_ratio'] = max_ratio\n",
    "            \n",
    "        return kernels\n",
    "    \n",
    "    def _build_batch(self,X,kernels,use_log_for_kernel_diff,inverse_kde,use_diff_kde,cpu_count,exclude_c_from_kde,exclude_g_from_kde):\n",
    "        batch_list = []\n",
    "        cols = self._num_features\n",
    "        if exclude_c_from_kde:\n",
    "            cols = [col for col in cols if not 'c-' in col]\n",
    "        if exclude_g_from_kde:\n",
    "            cols = [col for col in cols if not 'g-' in col]\n",
    "        col_size = len(cols)            \n",
    "\n",
    "        if col_size>=cpu_count:\n",
    "            batch_size = int(col_size/cpu_count)\n",
    "        else:\n",
    "            batch_size = 1\n",
    "            cpu_count = col_size\n",
    "        for i in range(cpu_count):\n",
    "            if i == cpu_count-1:\n",
    "                batch_list.append((cols[i*batch_size:],X,kernels,use_log_for_kernel_diff,inverse_kde,use_diff_kde))\n",
    "            else:\n",
    "                batch_list.append((cols[i*batch_size:(i+1)*batch_size],X,kernels,use_log_for_kernel_diff,inverse_kde,use_diff_kde))\n",
    "        return batch_list\n",
    "\n",
    "    def _process_individual_batch(self,batch):\n",
    "        ratio_multiplier = 10\n",
    "        cols = batch[0]\n",
    "        X = batch[1]\n",
    "        kernels = batch[2]\n",
    "        use_log_for_kernel_diff = batch[3]\n",
    "        inverse_kde = batch[4]\n",
    "        use_diff_kde = batch[5]\n",
    "        series_list = []\n",
    "        for col in cols:\n",
    "            kde_control_kernel = kernels[col+'_control']\n",
    "            kde_treatment_kernel = kernels[col+'_treatment']\n",
    "            \n",
    "            if use_diff_kde:\n",
    "                a_kde = kde_control_kernel.pdf(X[col].values)\n",
    "                b_kde = kde_treatment_kernel.pdf(X[col].values)\n",
    "                a = (b_kde-a_kde)/np.max((a_kde,b_kde),axis=0)\n",
    "                a = a.clip(-1,1)\n",
    "                a = np.nan_to_num(a,nan=0.0)\n",
    "            else:\n",
    "                if inverse_kde:\n",
    "                    a = kde_control_kernel.pdf(X[col].values)/kde_treatment_kernel.pdf(X[col].values)\n",
    "                else:\n",
    "                    a = kde_treatment_kernel.pdf(X[col].values)/kde_control_kernel.pdf(X[col].values)\n",
    "                a = np.nan_to_num(a,nan=ratio_multiplier*kernels[col+'_ratio'])\n",
    "                a = a.clip(0,ratio_multiplier*kernels[col+'_ratio'])\n",
    "                if use_log_for_kernel_diff:\n",
    "                    a = np.log1p(a)\n",
    "                    \n",
    "            a = pd.Series(a,name=col+'_kde_diff',dtype='float32')\n",
    "            series_list.append(a)\n",
    "        return series_list\n",
    "\n",
    "    def _run_batch(self,batch):\n",
    "        return self._process_individual_batch(batch)\n",
    "\n",
    "    def _process_batch_list(self,batch_list,cpu):\n",
    "        return joblib.Parallel(n_jobs=cpu)(joblib.delayed(self._run_batch)(batch) for batch in batch_list)\n",
    "\n",
    "    def _process_kde_parallelized(self,X,kernels,use_log_for_kernel_diff,inverse_kde,use_diff_kde,cpu,exclude_c_from_kde,exclude_g_from_kde):\n",
    "        batch_list = self._build_batch(X,kernels,use_log_for_kernel_diff,inverse_kde,use_diff_kde,cpu,exclude_c_from_kde,exclude_g_from_kde)\n",
    "        results = self._process_batch_list(batch_list,cpu)\n",
    "        for series_list in results:\n",
    "            for s in series_list:\n",
    "                X[s.name] = s.values\n",
    "        return X\n",
    "    \n",
    "    def _calculate_prob_dist_for_data_augmentation(self,X_list,granularity):\n",
    "        X = pd.concat(X_list)\n",
    "        X_treatment = X[X['cp_type']==0]\n",
    "        prob_dist = []\n",
    "        dist = np.arange(0,1,1/granularity)\n",
    "        for col in X.columns[4:]:\n",
    "            x = X_treatment[col].values\n",
    "            kernel = stats.gaussian_kde(x)\n",
    "            prob = kernel.pdf(dist)\n",
    "            prob_dist.append(prob)\n",
    "        prob_dist = np.array(prob_dist)\n",
    "        return prob_dist\n",
    "    \n",
    "    def _generate_data_augmentation(self,X,prob_dist,granularity,max_dev=0.1,normal_std_dev=0.1,additional=10):\n",
    "        rng = np.random.default_rng(seed=42)\n",
    "        x = X.values[:,4:].copy().astype(np.float16)\n",
    "\n",
    "        max_dev_steps = int(max_dev*granularity)\n",
    "\n",
    "        #Calculate normal matrix\n",
    "        normal_p = np.arange(-max_dev*granularity,max_dev*granularity+1,1)\n",
    "        normal_p = normal_p/granularity\n",
    "        normal_p = 1/(normal_std_dev)*np.exp(-(normal_p*normal_p)/normal_std_dev**2)\n",
    "        normal_p = normal_p.astype(np.float16)\n",
    "        normal_p = np.repeat(normal_p[np.newaxis,:], x.shape[1], axis=0)\n",
    "        normal_p = np.repeat(normal_p[np.newaxis,:,:], x.shape[0], axis=0)\n",
    "\n",
    "        #Transform x so that it rounds to the desired granularity\n",
    "        x_rounded = (np.round(x*granularity)).astype(int)\n",
    "\n",
    "\n",
    "        #For each and every value a in x, we want to calculate a vector of probability of size 2n+1 such as\n",
    "        #The probability value at index 0 is the probability that we remove max_dev to a\n",
    "        i_steps = np.arange(-max_dev_steps,max_dev_steps+1,1) #initialization vector for the steps\n",
    "        i_initial = np.tile(np.array([[i_steps]]),(x.shape[0],x.shape[1],1))\n",
    "        x_rounded_repeated = np.repeat(x_rounded[:, :, np.newaxis], i_steps.shape[0], axis=2)\n",
    "        idx = i_initial + x_rounded_repeated\n",
    "        idx = idx.copy()\n",
    "        idx = np.clip(idx,0,granularity-1) #For each \n",
    "        \n",
    "        del i_initial, x_rounded_repeated\n",
    "        gc.collect()\n",
    "\n",
    "        #prob_candidates = prob_dist[0,0,idx].copy()\n",
    "        prob_candidates = np.zeros(idx.shape)\n",
    "        for j in range(idx.shape[1]):\n",
    "            prob_candidates[:,j,:] = np.take(prob_dist[j,:],idx[:,j,:])\n",
    "        \n",
    "        del idx\n",
    "        gc.collect()\n",
    "        \n",
    "        prob_candidates = prob_candidates*normal_p\n",
    "        prob_candidates = prob_candidates.copy()\n",
    "\n",
    "        del normal_p\n",
    "        gc.collect()\n",
    "\n",
    "        prob_candidates = prob_candidates/prob_candidates.sum(axis=2)[:,:,np.newaxis]\n",
    "\n",
    "\n",
    "        var = np.zeros([x.shape[0],x.shape[1],additional])\n",
    "        i_steps_norm = i_steps/max_dev_steps*max_dev\n",
    "        print('calculating probas')\n",
    "        for k in range(x.shape[1]):\n",
    "            for i in range(x.shape[0]):\n",
    "                var[i,k,:] = np.random.choice(i_steps_norm,size=additional,p=prob_candidates[i,k,:])\n",
    "\n",
    "        var_list = []\n",
    "        for i in range(additional):\n",
    "            var_list.append(var[:,:,i].copy())\n",
    "        return var_list\n",
    "    \n",
    "    \n",
    "    def _calculate_g_cols_to_drop(self,cols,kde_kernels,g_removal_count):\n",
    "        g_to_drop = []\n",
    "        if g_removal_count==0:\n",
    "            return g_to_drop\n",
    "        g_cols = []\n",
    "        diff_list = []\n",
    "        for col in cols:\n",
    "            if 'g-'==col[:2]:\n",
    "                g_cols.append(col)\n",
    "                \n",
    "                kde_control_kernel = kde_kernels[col+'_control']\n",
    "                kde_treatment_kernel = kde_kernels[col+'_treatment']\n",
    "                \n",
    "                rg = np.arange(-10,10,0.1)\n",
    "                \n",
    "                vehicle_kde_sample = kde_treatment_kernel.pdf(rg)\n",
    "                control_kde_sample = kde_control_kernel.pdf(rg)\n",
    "                \n",
    "                diff = (np.abs(vehicle_kde_sample-control_kde_sample)).mean()\n",
    "                \n",
    "                diff_list.append(diff)\n",
    "\n",
    "        diff_list_ordered = np.sort(np.array(diff_list))\n",
    "        thresh = diff_list_ordered[g_removal_count-1]\n",
    "        \n",
    "        for col, diff in zip(g_cols,diff_list):\n",
    "            if diff<=thresh:\n",
    "                g_to_drop.append(col)\n",
    "                \n",
    "        return g_to_drop\n",
    "    \n",
    "    def _add_c_stats(self,X):\n",
    "        all_cols = X.columns\n",
    "        c_cols = [x for x in all_cols if ('c-' in x) & (not '_kde_diff' in x)]\n",
    "        X_values = X[c_cols].values\n",
    "        #Add stats\n",
    "        X['c_stats_sum'] = X_values.sum(axis=1)\n",
    "        X['c_stats_mean'] = X_values.mean(axis=1)\n",
    "        X['c_stats_median'] = np.median(X_values,axis=1)\n",
    "        X['c_stats_std'] = np.std(X_values,axis=1)\n",
    "        X['c_stats_kurtosis'] = stats.kurtosis(X_values,axis=1)\n",
    "        X['c_stats_skew'] = stats.skew(X_values,axis=1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _fit_pca(self,X_list,pca_for_kde,pca_for_c):\n",
    "        X = pd.concat(X_list,axis=0)\n",
    "        all_cols = X.columns\n",
    "        pca_cols = []\n",
    "        pca_names = ['g_pca']\n",
    "        if pca_for_c:\n",
    "            pca_names.append('c_pca')\n",
    "        pca_cols.append([x for x in all_cols if ('g-' in x) & (not '_kde_diff' in x)])\n",
    "        if pca_for_c:\n",
    "            pca_cols.append([x for x in all_cols if ('c-' in x) & (not '_kde_diff' in x) & (not '_stats' in x)])\n",
    "        if pca_for_kde:\n",
    "            pca_cols.append([x for x in all_cols if ('g-' in x) & ('_kde_diff' in x)])\n",
    "            if pca_for_c:\n",
    "                pca_cols.append([x for x in all_cols if ('c-' in x) & ('_kde_diff' in x) & (not '_stats' in x)])\n",
    "            pca_names.append('g_kde_pca')\n",
    "            if pca_for_c:\n",
    "                pca_names.append('c_kde_pca')\n",
    "\n",
    "        self.pca_cols_dict = {}\n",
    "        self.pca_dict = {}\n",
    "        for name,cols in zip(pca_names,pca_cols):\n",
    "            if len(cols)>0:\n",
    "                X_pca = X[cols]\n",
    "                pca = decomposition.PCA(n_components=X_pca.shape[1],\n",
    "                                          whiten=True,\n",
    "                                          svd_solver='full',\n",
    "                                          random_state=42\n",
    "                                         )\n",
    "                pca.fit(X_pca)\n",
    "                self.pca_cols_dict[name] = cols\n",
    "                self.pca_dict[name] = pca    \n",
    "                \n",
    "                \n",
    "    def _calculate_pca_components_to_keep(self,explained_variance_ratio_,pca_variance_threshold):\n",
    "        explained_variance_ratio_cum = explained_variance_ratio_.cumsum()\n",
    "        return np.argmax(explained_variance_ratio_cum>=pca_variance_threshold) + 1\n",
    "\n",
    "    def _transform_pca(self,X,pca_variance_threshold):\n",
    "        pca_names = list(self.pca_cols_dict.keys())\n",
    "        for name in pca_names:\n",
    "            #Recover cols and fit pca\n",
    "            cols = self.pca_cols_dict[name]\n",
    "            pca = self.pca_dict[name]\n",
    "\n",
    "            #Transform to current data\n",
    "            X_pca = pca.transform(X[cols])\n",
    "\n",
    "            #Keep only necessary data + transform into pd\n",
    "            variance_limit = self._calculate_pca_components_to_keep(pca.explained_variance_ratio_,pca_variance_threshold)\n",
    "            X_pca = X_pca[:,:variance_limit]\n",
    "            new_cols = [name+'_'+str(i) for i in range(variance_limit)]\n",
    "            X_pca = pd.DataFrame(X_pca,columns=new_cols)\n",
    "\n",
    "            #Adjust X\n",
    "            X.drop(cols,axis=1,inplace=True)\n",
    "            X = pd.concat([X,X_pca],axis=1)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def _duplicate_data_for_imbalanced_labels(self,X,y,folds):\n",
    "        cols_with_not_enough_data = np.where(y.iloc[:,1:].sum().values<folds)[0]\n",
    "        for col_index in cols_with_not_enough_data:\n",
    "            rows = np.where(y.iloc[:,col_index+1].values==1)[0]\n",
    "            n_rows = rows.shape[0]\n",
    "            if n_rows > 0:\n",
    "                n_duplicates = folds//n_rows + 1\n",
    "                X_duplicate_pd = X.iloc[rows,:].copy()\n",
    "                y_duplicate_pd = y.iloc[rows,:].copy()\n",
    "                X = pd.concat([X] + [X_duplicate_pd]*n_duplicates)\n",
    "                y = pd.concat([y] + [y_duplicate_pd]*n_duplicates)\n",
    "            \n",
    "        return X,y\n",
    "    \n",
    "    \n",
    "    def _add_nonscored_targets(self,X):\n",
    "        X = pd.merge(X,self.X_train_additional,on='sig_id')\n",
    "        y = X[self.X_train_additional.columns].copy()\n",
    "        return X,y\n",
    "\n",
    "    \n",
    "    def _pong_number(self,a,size):\n",
    "        divisor = a//size\n",
    "        remaining = a%size\n",
    "        is_even = divisor % 2 == 0\n",
    "        if is_even:\n",
    "            return remaining\n",
    "        else:\n",
    "            return size-remaining\n",
    "    \n",
    "    def _select_split_drugs(self,X,ratio):\n",
    "        treatment_drugs = X['drug_id'].values\n",
    "        drug_ids, drug_counts = np.unique(treatment_drugs, return_counts=True)\n",
    "        drug_id_counts = np.array([drug_ids,drug_counts]).transpose()\n",
    "        drug_id_counts = drug_id_counts[drug_id_counts[:,1].argsort()]\n",
    "        n_drugs = drug_id_counts.shape[0]\n",
    "        \n",
    "        #Randomly chose drugs that will appear only in holdout\n",
    "        holdout_drugs = []\n",
    "        array_size = X.shape[0]\n",
    "        limit = int(array_size*ratio)\n",
    "        count_drugs = 0\n",
    "        random_range = int(1/ratio)+1\n",
    "        counter = 0\n",
    "        \n",
    "        \n",
    "        while count_drugs<limit:\n",
    "            choice = random_range*counter + np.random.randint(0,random_range)\n",
    "            choice = self._pong_number(choice,n_drugs-1)\n",
    "            holdout_drugs.append(drug_id_counts[choice,0])\n",
    "            count_drugs += drug_id_counts[choice,1]\n",
    "            #holdout_drugs.append(np.take(drug_id_counts,choice,mode='wrap',axis=0)[0])\n",
    "            #count_drugs += np.take(drug_id_counts,choice,mode='wrap',axis=0)[1]\n",
    "            counter += 1\n",
    "            \n",
    "        return holdout_drugs\n",
    "            \n",
    "        \n",
    "    def split_train_holdout(self,validation_ratio=validation_ratio,seed=42,threshold=1000):\n",
    "        if validation_ratio==0:\n",
    "            X_train = self.X_train.copy()\n",
    "            X_train = pd.merge(X_train,self.y_train,on='sig_id')\n",
    "            X_train = pd.merge(X_train,self.X_train_drugs,on='sig_id')\n",
    "            X_drugs = X_train['drug_id'].values\n",
    "            X_sig_id = X_train['sig_id'].values\n",
    "            y_train = X_train[self.y_train.columns.tolist()].iloc[:,1:].copy()\n",
    "            X_train = X_train[self.X_train.columns.tolist()].iloc[:,1:].copy()\n",
    "            mask_train = np.isin(self.X_train['sig_id'].values,X_sig_id)\n",
    "            return  X_train,y_train,X_drugs,X_sig_id,None,None,mask_train\n",
    "        else:\n",
    "            seed_everything(seed)\n",
    "            X = self.X_train.copy()\n",
    "            X = pd.merge(X,self.y_train,on='sig_id')\n",
    "            X = pd.merge(X,self.X_train_drugs,on='sig_id')\n",
    "\n",
    "            #Split control in 2\n",
    "            mask_control = X.iloc[:,1].values==1\n",
    "            idx_control = self._boolean_to_index(mask_control)\n",
    "\n",
    "            idx_control_train, idx_control_holdout = train_test_split(idx_control,test_size=validation_ratio,random_state=seed)\n",
    "            X_control_train = X.iloc[idx_control_train]\n",
    "            X_control_holdout = X.iloc[idx_control_holdout]\n",
    "\n",
    "            #Identify drugs above threshold\n",
    "            drugs_count = X['drug_id'].value_counts()\n",
    "            drugs_above_thresh = drugs_count.loc[drugs_count>threshold].index.values.tolist()\n",
    "            drugs_above_thresh = list(set(drugs_above_thresh) - set(X[X['cp_type']==1]['drug_id'].values.tolist()))\n",
    "            if len(drugs_above_thresh)>0:\n",
    "                idx_drugs_spread = X[X['drug_id'].isin(drugs_above_thresh)].index.values.tolist()\n",
    "                mask_drugs_spread = self._index_to_boolean(idx_drugs_spread,X.shape[0])\n",
    "\n",
    "                idx_drugs_spread_train, idx_drugs_spread_holdout = train_test_split(idx_drugs_spread,test_size=validation_ratio,random_state=seed)\n",
    "                X_drugs_spread_train = X.iloc[idx_drugs_spread_train]\n",
    "                X_drugs_spread_holdout = X.iloc[idx_drugs_spread_holdout]\n",
    "\n",
    "            #Split treatment in 2 by respecting that drugs only appear on 1 group\n",
    "            #Create list of drugs and how many records they have\n",
    "            if len(drugs_above_thresh)>0:\n",
    "                mask_treatment = (X.iloc[:,1].values==0) & (mask_drugs_spread==False)\n",
    "            else:\n",
    "                mask_treatment = (X.iloc[:,1].values==0)\n",
    "            holdout_drugs = self._select_split_drugs(X[mask_treatment].copy(),validation_ratio)\n",
    "            \n",
    "            if len(drugs_above_thresh)==0:\n",
    "                X_train = pd.concat([X[~(X['drug_id'].isin(holdout_drugs)) & (X['cp_type']==0)],X_control_train],axis=0)\n",
    "                X_holdout = pd.concat([X[(X['drug_id'].isin(holdout_drugs)) & (X['cp_type']==0)],X_control_holdout],axis=0)\n",
    "            else:\n",
    "                X_train = pd.concat([X[~(X['drug_id'].isin(holdout_drugs)) & (X['cp_type']==0) & ~(X['drug_id'].isin(drugs_above_thresh))],X_control_train,X_drugs_spread_train],axis=0)\n",
    "                X_holdout = pd.concat([X[(X['drug_id'].isin(holdout_drugs)) & (X['cp_type']==0)],X_control_holdout,X_drugs_spread_holdout],axis=0)\n",
    "            \n",
    "            #reorder\n",
    "            X_train = pd.merge(self.X_train['sig_id'],X_train,on=['sig_id'])\n",
    "            \n",
    "            y_train = X_train[self.y_train.columns.tolist()].iloc[:,1:].copy()\n",
    "            y_holdout = X_holdout[self.y_train.columns.tolist()].iloc[:,1:].copy()\n",
    "            \n",
    "            X_drugs = X_train['drug_id'].values\n",
    "            X_sig_id = X_train['sig_id'].values\n",
    "            X_train = X_train[self.X_train.columns.tolist()].iloc[:,1:].copy()\n",
    "            X_holdout = X_holdout[self.X_train.columns.tolist()].iloc[:,1:].copy()\n",
    "            mask_train = np.isin(self.X_train['sig_id'].values,X_sig_id)\n",
    "            \n",
    "            return X_train,y_train,X_drugs,X_sig_id,X_holdout,y_holdout,mask_train\n",
    "        \n",
    "    def create_cv(self,X,y,drugs,sig_ids,threshold=1000,folds=folds,seed=42):\n",
    "        seed_everything(seed)\n",
    "\n",
    "        y_cols = y.columns.tolist()\n",
    "        X = X.copy()\n",
    "        y = y.copy()\n",
    "\n",
    "        X = pd.concat([X,y],axis=1)\n",
    "        X['drug_id'] = drugs\n",
    "        X['sig_id'] = sig_ids\n",
    "\n",
    "        #Locate drugs\n",
    "        drugs_count = X['drug_id'].value_counts()\n",
    "        drugs_below_thresh = drugs_count.loc[drugs_count<=threshold].index.sort_values()\n",
    "        drugs_above_thresh = drugs_count.loc[drugs_count>threshold].index.sort_values()\n",
    "\n",
    "        dct_below_thresh = {}; dct_above_thresh = {}\n",
    "        #Stratify below threshold\n",
    "        skf= MultilabelStratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        tmp = X.groupby('drug_id')[y_cols].mean().loc[drugs_below_thresh]\n",
    "        for f,(idxT,idxV) in enumerate( skf.split(tmp,tmp[y_cols])):\n",
    "            dd = {k:f for k in tmp.index[idxV].values}\n",
    "            dct_below_thresh.update(dd)\n",
    "\n",
    "        #stratify above threshold\n",
    "        skf= MultilabelStratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        tmp = X.loc[X['drug_id'].isin(drugs_above_thresh)].reset_index(drop=True)\n",
    "        for f,(idxT,idxV) in enumerate( skf.split(tmp,tmp[y_cols])):\n",
    "            dd = {k:f for k in tmp.sig_id[idxV].values}\n",
    "            dct_above_thresh.update(dd)\n",
    "\n",
    "        # ASSIGN FOLDS\n",
    "        X['fold'] = X['drug_id'].map(dct_below_thresh)\n",
    "        X.loc[X['fold'].isna(),'fold'] = X.loc[X['fold'].isna(),'sig_id'].map(dct_above_thresh)\n",
    "        X['fold'] = X['fold'].astype('int8')\n",
    "        \n",
    "        oof_assignment = X['fold'].values\n",
    "        \n",
    "        oof_idx = []\n",
    "        for x in np.arange(folds):\n",
    "            train = np.where(oof_assignment!=x)[0]\n",
    "            val = np.where(oof_assignment==x)[0]\n",
    "            oof_idx.append((train,val))\n",
    "        return oof_idx\n",
    "    \n",
    "    def create_cv_kfold(self,X,drugs,sig_ids,folds=folds,seed=42):\n",
    "        seed_everything(seed)\n",
    "        \n",
    "        X = X.copy()\n",
    "\n",
    "        X['drug_id'] = drugs\n",
    "        X['sig_id'] = sig_ids\n",
    "        \n",
    "        mask_treatment = X.iloc[:,0].values==0\n",
    "        mask_control = X.iloc[:,0].values==1\n",
    "        X_with_treatment = X[mask_treatment].copy()\n",
    "        X_control = X[mask_control].copy()\n",
    "        \n",
    "        drugs_dict = {}\n",
    "        drugs_already_in_fold = []\n",
    "        for i in range(folds):\n",
    "            X_remaining = X_with_treatment[~X_with_treatment['drug_id'].isin(drugs_already_in_fold)].copy()\n",
    "\n",
    "            if i<folds-1:\n",
    "                tmp_drugs = self._select_split_drugs(X_remaining,1.0/(folds-i))\n",
    "                drugs_already_in_fold = drugs_already_in_fold + tmp_drugs\n",
    "                dd = {k:i for k in tmp_drugs}\n",
    "                drugs_dict.update(dd)\n",
    "            else:\n",
    "                dd = {k:i for k in X_remaining['drug_id'].values.tolist()}\n",
    "                drugs_dict.update(dd)\n",
    "                \n",
    "        \n",
    "        X_with_treatment['fold'] = X_with_treatment['drug_id'].map(drugs_dict)\n",
    "        X_control['fold'] = np.random.randint(0,folds,size=X_control.shape[0])\n",
    "        \n",
    "        X_all = pd.concat([X_with_treatment,X_control],axis=0)\n",
    "        \n",
    "        X_all = pd.merge(X[['sig_id']],X_all,on='sig_id')\n",
    "        \n",
    "        oof_assignment = X_all['fold'].values\n",
    "        \n",
    "        oof_idx = []\n",
    "        for x in np.arange(folds):\n",
    "            train = np.where(oof_assignment!=x)[0]\n",
    "            val = np.where(oof_assignment==x)[0]\n",
    "            oof_idx.append((train,val))\n",
    "        return oof_idx\n",
    "    \n",
    "    \n",
    "def add_control_test_to_train(prepared_data):\n",
    "    X_test = prepared_data.X_test\n",
    "    X_train = prepared_data.X_train\n",
    "    y_train = prepared_data.y_train\n",
    "    \n",
    "    X_test_control = X_test[X_test['cp_type']==1]\n",
    "    X_train = pd.concat([X_train,X_test_control],axis=0)\n",
    "\n",
    "    y_test_control = pd.DataFrame(np.zeros((X_test_control.shape[0],y_train.shape[1])),columns=y_train.columns.tolist())\n",
    "    y_train = pd.concat([y_train,y_test_control],axis=0)\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nn_architecture == 'v0':\n",
    "    class TabularNN(nn.Module):\n",
    "        def __init__(self, cfg,last_layer_bias=None):\n",
    "            super().__init__()\n",
    "\n",
    "            self.cfg = cfg\n",
    "\n",
    "            self.batchnorm0 = nn.BatchNorm1d(cfg.num_features)\n",
    "\n",
    "            self.dense1 = nn.utils.weight_norm(nn.Linear(cfg.num_features, cfg.hidden_size_layer1))\n",
    "            self.prelu1 = nn.PReLU()\n",
    "            self.batchnorm1 = nn.BatchNorm1d(cfg.hidden_size_layer1)\n",
    "            self.dropout1 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense2 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer1, cfg.hidden_size_layer2))\n",
    "            self.prelu2 = nn.PReLU()\n",
    "            self.batchnorm2 = nn.BatchNorm1d(cfg.hidden_size_layer2)\n",
    "            self.dropout2 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense3 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer2, cfg.target_cols))\n",
    "\n",
    "\n",
    "            if not last_layer_bias is None:\n",
    "                last_layer_bias = np.log(last_layer_bias.mean(axis=0).clamp(1e-10,1))\n",
    "                self.dense3.bias.data = nn.Parameter(last_layer_bias.float())\n",
    "\n",
    "            self.params = []\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm0,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense1,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu1,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm1,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout1,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense2,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu2,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm2,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout2,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense3,none_to_zero=True)\n",
    "\n",
    "\n",
    "        def set_bias_weight_decay(self,layer,none_to_zero=False,all_to_zero=False):\n",
    "            params = []\n",
    "            named_params = dict(layer.named_parameters())\n",
    "            for key, value in named_params.items():\n",
    "                if none_to_zero:\n",
    "                    params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "                else:\n",
    "                    if key == 'bias':\n",
    "                        params += [{'params':value,'weight_decay':0.0}]\n",
    "                    else:\n",
    "                        if all_to_zero:\n",
    "                            params += [{'params':value,'weight_decay':0.0}]\n",
    "                        else:\n",
    "                            params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "            return params\n",
    "\n",
    "        def recalibrate_layer(self, layer):\n",
    "            if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "                layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "                layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "            if(torch.isnan(layer.weight).sum() > 0):\n",
    "                layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "                layer.weight += 1e-7\n",
    "\n",
    "        def forward(self, x):\n",
    "            x0 = self.batchnorm0(x)\n",
    "\n",
    "            self.recalibrate_layer(self.dense1)\n",
    "            x1 = self.dense1(x0)\n",
    "            x1 = self.prelu1(x1)\n",
    "            x1 = self.batchnorm1(x1)\n",
    "            x1 = self.dropout1(x1)\n",
    "\n",
    "            self.recalibrate_layer(self.dense2)\n",
    "            x2 = self.dense2(x1)\n",
    "            x2 = self.prelu2(x2)\n",
    "            x2 = self.batchnorm2(x2)\n",
    "            x2 = self.dropout2(x2)\n",
    "\n",
    "            self.recalibrate_layer(self.dense3)\n",
    "            y = self.dense3(x2)\n",
    "            return y\n",
    "\n",
    "    class CFG:\n",
    "        hidden_size_layer1=2048\n",
    "        hidden_size_layer2=2048\n",
    "        dropout=0.4\n",
    "        weight_decay=1e-5\n",
    "        batch_size=128\n",
    "        epochs=100\n",
    "        min_epochs = 25\n",
    "        one_cycle_epochs=25\n",
    "        number_one_cycle=1\n",
    "        early_stopping=20\n",
    "        learning_rate=1e-3\n",
    "        patience=15\n",
    "        hard_patience=25\n",
    "        min_delta=0.00005\n",
    "        ratio_train_val=1.15\n",
    "        pct_start=0.1\n",
    "        div_factor=1e3\n",
    "        verbose=1\n",
    "        \n",
    "elif nn_architecture == 'v1':\n",
    "    class TabularNN(nn.Module):\n",
    "        def __init__(self, cfg,last_layer_bias=None):\n",
    "            super().__init__()\n",
    "\n",
    "            self.cfg = cfg\n",
    "\n",
    "            self.batchnorm0 = nn.BatchNorm1d(cfg.num_features)\n",
    "\n",
    "            self.dense1 = nn.utils.weight_norm(nn.Linear(cfg.num_features, cfg.hidden_size_layer1))\n",
    "            self.prelu1 = nn.PReLU()\n",
    "            self.batchnorm1 = nn.BatchNorm1d(cfg.hidden_size_layer1)\n",
    "            self.dropout1 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense2 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer1, cfg.hidden_size_layer2))\n",
    "            self.prelu2 = nn.PReLU()\n",
    "            self.batchnorm2 = nn.BatchNorm1d(cfg.hidden_size_layer2)\n",
    "            self.dropout2 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense3 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer2, cfg.target_cols))\n",
    "\n",
    "\n",
    "            if not last_layer_bias is None:\n",
    "                last_layer_bias = np.log(last_layer_bias.mean(axis=0).clamp(1e-10,1))\n",
    "                self.dense3.bias.data = nn.Parameter(last_layer_bias.float())\n",
    "\n",
    "            self.params = []\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm0,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense1)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu1,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm1,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout1,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense2)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu2,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm2,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout2,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense3)\n",
    "\n",
    "\n",
    "        def set_bias_weight_decay(self,layer,none_to_zero=False,all_to_zero=False):\n",
    "            params = []\n",
    "            named_params = dict(layer.named_parameters())\n",
    "            for key, value in named_params.items():\n",
    "                if none_to_zero:\n",
    "                    params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "                else:\n",
    "                    if key == 'bias':\n",
    "                        params += [{'params':value,'weight_decay':0.0}]\n",
    "                    else:\n",
    "                        if all_to_zero:\n",
    "                            params += [{'params':value,'weight_decay':0.0}]\n",
    "                        else:\n",
    "                            params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "            return params\n",
    "\n",
    "        def recalibrate_layer(self, layer):\n",
    "            if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "                layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "                layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "            if(torch.isnan(layer.weight).sum() > 0):\n",
    "                layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "                layer.weight += 1e-7\n",
    "\n",
    "        def forward(self, x):\n",
    "            x0 = self.batchnorm0(x)\n",
    "\n",
    "            self.recalibrate_layer(self.dense1)\n",
    "            x1 = self.dense1(x0)\n",
    "            x1 = self.prelu1(x1)\n",
    "            x1 = self.batchnorm1(x1)\n",
    "            x1 = self.dropout1(x1)\n",
    "\n",
    "            self.recalibrate_layer(self.dense2)\n",
    "            x2 = self.dense2(x1)\n",
    "            x2 = self.prelu2(x2)\n",
    "            x2 = self.batchnorm2(x2)\n",
    "            x2 = self.dropout2(x2)\n",
    "\n",
    "            self.recalibrate_layer(self.dense3)\n",
    "            y = self.dense3(x2)\n",
    "            return y\n",
    "\n",
    "    class CFG:\n",
    "        hidden_size_layer1=2048\n",
    "        hidden_size_layer2=2048\n",
    "        dropout=0.4\n",
    "        weight_decay=1e-5\n",
    "        batch_size=128\n",
    "        epochs=100\n",
    "        min_epochs = 25\n",
    "        one_cycle_epochs=25\n",
    "        number_one_cycle=1\n",
    "        early_stopping=20\n",
    "        learning_rate=1e-3\n",
    "        patience=15\n",
    "        hard_patience=25\n",
    "        min_delta=0.00005\n",
    "        ratio_train_val=1.15\n",
    "        pct_start=0.1\n",
    "        div_factor=1e3\n",
    "        verbose=1\n",
    "\n",
    "elif nn_architecture == 'v1.1':\n",
    "    class TabularNN(nn.Module):\n",
    "        def __init__(self, cfg,last_layer_bias=None):\n",
    "            super().__init__()\n",
    "\n",
    "            self.cfg = cfg\n",
    "\n",
    "            self.batchnorm0 = nn.BatchNorm1d(cfg.num_features)\n",
    "\n",
    "            self.dense1 = nn.utils.weight_norm(nn.Linear(cfg.num_features, cfg.hidden_size_layer1))\n",
    "            self.prelu1 = nn.PReLU()\n",
    "            self.batchnorm1 = nn.BatchNorm1d(cfg.hidden_size_layer1)\n",
    "            self.dropout1 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense2 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer1, cfg.hidden_size_layer2))\n",
    "            self.prelu2 = nn.PReLU()\n",
    "            self.batchnorm2 = nn.BatchNorm1d(cfg.hidden_size_layer2)\n",
    "            self.dropout2 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense3 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer2, cfg.target_cols))\n",
    "\n",
    "\n",
    "            if not last_layer_bias is None:\n",
    "                last_layer_bias = np.log(last_layer_bias.mean(axis=0).clamp(1e-10,1))\n",
    "                self.dense3.bias.data = nn.Parameter(last_layer_bias.float())\n",
    "\n",
    "            self.params = []\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm0,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense1)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu1,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm1,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout1,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense2)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu2,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm2,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout2,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense3)\n",
    "\n",
    "\n",
    "        def set_bias_weight_decay(self,layer,none_to_zero=False,all_to_zero=False):\n",
    "            params = []\n",
    "            named_params = dict(layer.named_parameters())\n",
    "            for key, value in named_params.items():\n",
    "                if none_to_zero:\n",
    "                    params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "                else:\n",
    "                    if key == 'bias':\n",
    "                        params += [{'params':value,'weight_decay':0.0}]\n",
    "                    else:\n",
    "                        if all_to_zero:\n",
    "                            params += [{'params':value,'weight_decay':0.0}]\n",
    "                        else:\n",
    "                            params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "            return params\n",
    "\n",
    "        def recalibrate_layer(self, layer):\n",
    "            if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "                layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "                layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "            if(torch.isnan(layer.weight).sum() > 0):\n",
    "                layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "                layer.weight += 1e-7\n",
    "\n",
    "        def forward(self, x):\n",
    "            x0 = self.batchnorm0(x)\n",
    "\n",
    "            self.recalibrate_layer(self.dense1)\n",
    "            x1 = self.dense1(x0)\n",
    "            x1 = self.prelu1(x1)\n",
    "            x1 = self.batchnorm1(x1)\n",
    "            x1 = self.dropout1(x1)\n",
    "\n",
    "            self.recalibrate_layer(self.dense2)\n",
    "            x2 = self.dense2(x1)\n",
    "            x2 = self.prelu2(x2)\n",
    "            x2 = self.batchnorm2(x2)\n",
    "            x2 = self.dropout2(x2)\n",
    "\n",
    "            self.recalibrate_layer(self.dense3)\n",
    "            y = self.dense3(x2)\n",
    "            return y\n",
    "\n",
    "    class CFG:\n",
    "        hidden_size_layer1=2048\n",
    "        hidden_size_layer2=2048\n",
    "        dropout=0.4\n",
    "        weight_decay=1e-5\n",
    "        batch_size=128\n",
    "        epochs=120\n",
    "        min_epochs = 25\n",
    "        one_cycle_epochs=50\n",
    "        number_one_cycle=1\n",
    "        early_stopping=30\n",
    "        learning_rate=1e-3\n",
    "        patience=15\n",
    "        hard_patience=25\n",
    "        min_delta=0.00005\n",
    "        ratio_train_val=1.15\n",
    "        pct_start=0.1\n",
    "        div_factor=1e3\n",
    "        verbose=1\n",
    "        \n",
    "elif nn_architecture == 'v1.2':\n",
    "    class TabularNN(nn.Module):\n",
    "        def __init__(self, cfg,last_layer_bias=None):\n",
    "            super().__init__()\n",
    "\n",
    "            self.cfg = cfg\n",
    "\n",
    "            self.batchnorm0 = nn.BatchNorm1d(cfg.num_features)\n",
    "\n",
    "            self.dense1 = nn.utils.weight_norm(nn.Linear(cfg.num_features, cfg.hidden_size_layer1))\n",
    "            self.prelu1 = nn.PReLU()\n",
    "            self.batchnorm1 = nn.BatchNorm1d(cfg.hidden_size_layer1)\n",
    "            self.dropout1 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense2 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer1, cfg.hidden_size_layer2))\n",
    "            self.prelu2 = nn.PReLU()\n",
    "            self.batchnorm2 = nn.BatchNorm1d(cfg.hidden_size_layer2)\n",
    "            self.dropout2 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense3 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer2, cfg.target_cols))\n",
    "\n",
    "\n",
    "            if not last_layer_bias is None:\n",
    "                last_layer_bias = np.log(last_layer_bias.mean(axis=0).clamp(1e-10,1))\n",
    "                self.dense3.bias.data = nn.Parameter(last_layer_bias.float())\n",
    "\n",
    "            self.params = []\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm0,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense1)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu1,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm1,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout1,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense2)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu2,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm2,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout2,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense3)\n",
    "\n",
    "\n",
    "        def set_bias_weight_decay(self,layer,none_to_zero=False,all_to_zero=False):\n",
    "            params = []\n",
    "            named_params = dict(layer.named_parameters())\n",
    "            for key, value in named_params.items():\n",
    "                if none_to_zero:\n",
    "                    params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "                else:\n",
    "                    if key == 'bias':\n",
    "                        params += [{'params':value,'weight_decay':0.0}]\n",
    "                    else:\n",
    "                        if all_to_zero:\n",
    "                            params += [{'params':value,'weight_decay':0.0}]\n",
    "                        else:\n",
    "                            params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "            return params\n",
    "\n",
    "        def recalibrate_layer(self, layer):\n",
    "            if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "                layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "                layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "            if(torch.isnan(layer.weight).sum() > 0):\n",
    "                layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "                layer.weight += 1e-7\n",
    "\n",
    "        def forward(self, x):\n",
    "            x0 = self.batchnorm0(x)\n",
    "\n",
    "            self.recalibrate_layer(self.dense1)\n",
    "            x1 = self.dense1(x0)\n",
    "            x1 = self.prelu1(x1)\n",
    "            x1 = self.batchnorm1(x1)\n",
    "            x1 = self.dropout1(x1)\n",
    "\n",
    "            self.recalibrate_layer(self.dense2)\n",
    "            x2 = self.dense2(x1)\n",
    "            x2 = self.prelu2(x2)\n",
    "            x2 = self.batchnorm2(x2)\n",
    "            x2 = self.dropout2(x2)\n",
    "\n",
    "            self.recalibrate_layer(self.dense3)\n",
    "            y = self.dense3(x2)\n",
    "            return y\n",
    "\n",
    "    class CFG:\n",
    "        hidden_size_layer1=2048\n",
    "        hidden_size_layer2=2048\n",
    "        dropout=0.4\n",
    "        weight_decay=1e-5\n",
    "        batch_size=128\n",
    "        epochs=120\n",
    "        min_epochs = 40\n",
    "        one_cycle_epochs=100\n",
    "        number_one_cycle=1\n",
    "        early_stopping=30\n",
    "        learning_rate=1e-3\n",
    "        patience=30\n",
    "        hard_patience=25\n",
    "        min_delta=0.00005\n",
    "        ratio_train_val=1.15\n",
    "        pct_start=0.1\n",
    "        div_factor=1e3\n",
    "        verbose=1\n",
    "\n",
    "elif nn_architecture == 'v2':\n",
    "    class TabularNN(nn.Module):\n",
    "        def __init__(self, cfg,last_layer_bias=None):\n",
    "            super().__init__()\n",
    "\n",
    "            self.cfg = cfg\n",
    "\n",
    "            self.batchnorm0 = nn.BatchNorm1d(cfg.num_features)\n",
    "\n",
    "            self.dense1 = nn.utils.weight_norm(nn.Linear(cfg.num_features, cfg.hidden_size_layer1))\n",
    "            self.prelu1 = nn.PReLU()\n",
    "            self.batchnorm1 = nn.BatchNorm1d(cfg.hidden_size_layer1)\n",
    "            self.dropout1 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense2 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer1, cfg.hidden_size_layer2))\n",
    "            self.prelu2 = nn.PReLU()\n",
    "            self.batchnorm2 = nn.BatchNorm1d(cfg.hidden_size_layer2)\n",
    "            self.dropout2 = nn.Dropout(cfg.dropout)\n",
    "\n",
    "            self.dense3 = nn.utils.weight_norm(nn.Linear(cfg.hidden_size_layer2, cfg.target_cols))\n",
    "\n",
    "\n",
    "            if not last_layer_bias is None:\n",
    "                last_layer_bias = np.log(last_layer_bias.mean(axis=0).clamp(1e-10,1))\n",
    "                self.dense3.bias.data = nn.Parameter(last_layer_bias.float())\n",
    "\n",
    "            self.params = []\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm0,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense1)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu1,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm1,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout1,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense2)\n",
    "            self.params += self.set_bias_weight_decay(self.prelu2,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.batchnorm2,none_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dropout2,all_to_zero=True)\n",
    "            self.params += self.set_bias_weight_decay(self.dense3)\n",
    "\n",
    "\n",
    "        def set_bias_weight_decay(self,layer,none_to_zero=False,all_to_zero=False):\n",
    "            params = []\n",
    "            named_params = dict(layer.named_parameters())\n",
    "            for key, value in named_params.items():\n",
    "                if none_to_zero:\n",
    "                    params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "                else:\n",
    "                    if key == 'bias':\n",
    "                        params += [{'params':value,'weight_decay':0.0}]\n",
    "                    else:\n",
    "                        if all_to_zero:\n",
    "                            params += [{'params':value,'weight_decay':0.0}]\n",
    "                        else:\n",
    "                            params += [{'params':value,'weight_decay':self.cfg.weight_decay}]\n",
    "            return params\n",
    "\n",
    "        def recalibrate_layer(self, layer):\n",
    "            if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "                layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "                layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "            if(torch.isnan(layer.weight).sum() > 0):\n",
    "                layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "                layer.weight += 1e-7\n",
    "\n",
    "        def forward(self, x):\n",
    "            x0 = self.batchnorm0(x)\n",
    "\n",
    "            self.recalibrate_layer(self.dense1)\n",
    "            x1 = self.dense1(x0)\n",
    "            x1 = self.prelu1(x1)\n",
    "            x1 = self.batchnorm1(x1)\n",
    "            x1 = self.dropout1(x1)\n",
    "\n",
    "            self.recalibrate_layer(self.dense2)\n",
    "            x2 = self.dense2(x1)\n",
    "            x2 = self.prelu2(x2)\n",
    "            x2 = self.batchnorm2(x2)\n",
    "            x2 = self.dropout2(x2)\n",
    "\n",
    "            self.recalibrate_layer(self.dense3)\n",
    "            y = self.dense3(x2)\n",
    "            return y\n",
    "\n",
    "    class CFG:\n",
    "        hidden_size_layer1=1024\n",
    "        hidden_size_layer2=1024\n",
    "        dropout=0.4\n",
    "        weight_decay=1e-5\n",
    "        batch_size=128\n",
    "        epochs=100\n",
    "        min_epochs = 25\n",
    "        one_cycle_epochs=25\n",
    "        number_one_cycle=1\n",
    "        early_stopping=20\n",
    "        learning_rate=1e-3\n",
    "        patience=15\n",
    "        hard_patience=25\n",
    "        min_delta=0.00005\n",
    "        ratio_train_val=1.15\n",
    "        pct_start=0.1\n",
    "        div_factor=1e3\n",
    "        verbose=1\n",
    "    \n",
    "    \n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "    def forward(self, x, target):\n",
    "        confidence = 1. - self.smoothing\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        bcs_loss = nn.BCEWithLogitsLoss()(x, target)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * bcs_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "    \n",
    "    \n",
    "class NNWrapper():\n",
    "\n",
    "    def __init__(self,model_class,cfg):\n",
    "        self.model_class = model_class\n",
    "        self.cfg = cfg\n",
    "        self.clamp = 1e-7 #To avoid having 0 or 1 in logloss\n",
    "        self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def _evaluate(self,y_actual,y_pred):\n",
    "        if set([0,1])==set(self.labels):\n",
    "            return metrics.log_loss(y_actual.reshape(-1,1),y_pred.reshape(-1,1),labels=self.labels)\n",
    "        else:\n",
    "            return -np.sum(y_actual.reshape(-1,1)*np.log(y_pred.reshape(-1,1)) + (1-y_actual.reshape(-1,1))*np.log(1-y_pred.reshape(-1,1)))/y_pred.reshape(-1,1).shape[0]\n",
    "        \n",
    "    def _numpy_to_tensor(self,n):\n",
    "        t = (torch.from_numpy(n)).float()\n",
    "        return t\n",
    "    \n",
    "    def _index_to_boolean(self,idx,size):\n",
    "            mask_array = np.zeros(size)\n",
    "            mask_array = mask_array==1\n",
    "            mask_array[idx] = True\n",
    "            return mask_array\n",
    "        \n",
    "    def fit(self,X,y,X_holdout=None,y_holdout=None,folds=5,params=None,evaluate=True,oof_idx=None,seed=42,moa_control_params=None):\n",
    "        seed_everything(seed=seed)\n",
    "        #Information on data\n",
    "        self.labels = list(np.unique(y))\n",
    "        \n",
    "        \n",
    "        #Parameters particular to MoA\n",
    "        self.moa_control_params = moa_control_params\n",
    "        \n",
    "        \n",
    "        if oof_idx is None:\n",
    "            self._oof_idx = []\n",
    "            if (y.ndim == 2) and (y.shape[1] > 1):\n",
    "                cv = KFold(n_splits=folds, shuffle=True, random_state=SEED)\n",
    "                if not self.moa_control_params is None:\n",
    "                    if ['fold_type']=='multi':\n",
    "                        cv = MultilabelStratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n",
    "            else:\n",
    "                cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n",
    "            for (train_idx, val_idx) in cv.split(X,y):\n",
    "                self._oof_idx.append((train_idx,val_idx))\n",
    "        else:\n",
    "            self._oof_idx = oof_idx\n",
    "        \n",
    "        #Train models\n",
    "        self.cv_models = []\n",
    "        self._X = X.copy()\n",
    "        self.total_oof_loss = 0\n",
    "        self._y = y\n",
    "        for fold, (train_idx, val_idx) in enumerate(self._oof_idx):\n",
    "            print(\"Start training fold {} of {}\".format(fold+1,folds))\n",
    "            if self.moa_control_params is None:\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "            else:\n",
    "                train_mask = self._index_to_boolean(train_idx,X.shape[0])\n",
    "                val_mask = self._index_to_boolean(val_idx,X.shape[0])\n",
    "                \n",
    "                train_mask_non_control = train_mask & self.moa_control_params['mask_treatment']\n",
    "                self.train_mask_non_control = train_mask_non_control\n",
    "                train_mask_control = train_mask & (self.moa_control_params['mask_treatment']==False)\n",
    "                val_mask_non_control = val_mask & self.moa_control_params['mask_treatment']\n",
    "                \n",
    "                X_train, X_val = X[train_mask_non_control], X[val_mask_non_control]\n",
    "                y_train, y_val = y[train_mask_non_control], y[val_mask_non_control]\n",
    "                \n",
    "                if self.moa_control_params['add_control_from_test']:\n",
    "                    self.X_control = np.concatenate([X[train_mask_control],self.moa_control_params['test_control_data']],axis=0)\n",
    "                else:\n",
    "                    self.X_control = X[train_mask_control]\n",
    "            \n",
    "            model = self._train(X_train,y_train,X_val,y_val)\n",
    "            self.cv_models.append(model)\n",
    "            \n",
    "            if evaluate:\n",
    "                #Evaluate fold model\n",
    "                oof_preds = self._predict_proba_model(model,X_val)\n",
    "                oof_loss = self._evaluate(y_val,oof_preds)\n",
    "                self.total_oof_loss = self.total_oof_loss + oof_loss/folds\n",
    "                if not X_holdout is None:\n",
    "                    holdout_preds = self._predict_proba_model(model,X_holdout)\n",
    "                    holdout_loss = self._evaluate(y_holdout,holdout_preds)\n",
    "\n",
    "                if not X_holdout is None:\n",
    "                    print('Fold {} out of fold score: {:.6f}; holdout score: {:.6f}'.format(fold+1,oof_loss,holdout_loss))\n",
    "                else:\n",
    "                    print('Fold {} out of fold score: {:.6f}'.format(fold+1,oof_loss))\n",
    "             \n",
    "        #Evaluate whole model\n",
    "        if evaluate:\n",
    "            if not X_holdout is None:\n",
    "                y_pred_holdout = self.predict(X_holdout)\n",
    "                holdout_loss = self._evaluate(params,y_holdout,y_pred_holdout)\n",
    "            if not X_holdout is None:\n",
    "                print('Total oof score: {:.6f}; holdout score: {:.6f}'.format(self.total_oof_loss,holdout_loss))\n",
    "            else:\n",
    "                print('Total oof score: {:.6f}'.format(self.total_oof_loss))\n",
    "\n",
    "            \n",
    "    def _prepare_batch_data(self,X,y=None,inference=False):\n",
    "        if inference:\n",
    "            dataset = DataLoader(TensorDataset(X),batch_size=X.shape[0],shuffle=False,pin_memory=True,num_workers=multiprocessing.cpu_count()-1)\n",
    "        else:\n",
    "            #Before passing it to DataLoader, X,y need to be in the following dimension: items, features\n",
    "            dataset = DataLoader(TensorDataset(X,y),batch_size=self.cfg.batch_size,shuffle=True,pin_memory=True,num_workers=multiprocessing.cpu_count()-1,drop_last=True)\n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    def _calculate_controle_size_to_add(self,X_train):\n",
    "        #Calculate how many we take from control pool\n",
    "        control_size_to_add = min(math.floor(X_train.shape[0]*self.moa_control_params['control_share']),self.X_control.shape[0])\n",
    "        print('Add {} control to training set, representing {:.0f}% of vehicle size'.format(control_size_to_add,100*control_size_to_add/X_train.shape[0]))\n",
    "        return control_size_to_add\n",
    "    \n",
    "    def _moa_add_control_data(self,X_train,y_train,control_size_to_add):\n",
    "        \n",
    "        if control_size_to_add>0:\n",
    "            mask_control_to_add = np.random.choice(np.arange(0,self.X_control.shape[0]), size=control_size_to_add, replace=False)\n",
    "            X_control_to_add = self.X_control[mask_control_to_add]\n",
    "            X_train = np.concatenate([X_train,X_control_to_add],axis=0)\n",
    "\n",
    "            y_control_to_add = np.zeros((X_control_to_add.shape[0],y_train.shape[1]),dtype=y_train.dtype)\n",
    "            y_train = np.concatenate([y_train,y_control_to_add],axis=0)\n",
    "        \n",
    "        X_train_torch = self._numpy_to_tensor(X_train)\n",
    "        y_train_torch = self._numpy_to_tensor(y_train)\n",
    "        \n",
    "        return X_train_torch,y_train_torch\n",
    "    \n",
    "    \n",
    "    def _augment_data(self,X,var_list,mask,epoch):\n",
    "        var_size = len(var_list)\n",
    "        i = (epoch-1)%var_size\n",
    "        var = var_list[i]\n",
    "        var_val = var[mask]\n",
    "        X = X + var_val\n",
    "        return X\n",
    "        \n",
    "            \n",
    "    def _train(self, X_train, y_train, X_val, y_val):\n",
    "        initial_time = datetime.datetime.now()\n",
    "        \n",
    "        X_train_size = X_train.shape[0]\n",
    "\n",
    "        if not self.moa_control_params is None:\n",
    "            control_size_to_add = self._calculate_controle_size_to_add(X_train)\n",
    "            X_train_size += control_size_to_add\n",
    "        \n",
    "        #Format data\n",
    "        X_train_torch = self._numpy_to_tensor(X_train)\n",
    "        y_train_torch = self._numpy_to_tensor(y_train)\n",
    "        X_val_torch = self._numpy_to_tensor(X_val)\n",
    "        y_val_torch = self._numpy_to_tensor(y_val)\n",
    "        \n",
    "        #Initialize model\n",
    "        model = self.model_class(self.cfg,y_train_torch)\n",
    "        model.to(self.device)\n",
    "        \n",
    "        #Initialize model parameters\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "        loss_val_fn = torch.nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "        \n",
    "        if not self.moa_control_params is None:\n",
    "            if self.moa_control_params['label_smoothing']>0:\n",
    "                loss_fn = LabelSmoothingCrossEntropy(self.moa_control_params['label_smoothing'])\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.params, lr=self.cfg.learning_rate, weight_decay=self.cfg.weight_decay)\n",
    "        \n",
    "        \n",
    "        total_steps_oneCycle = (int(X_train_size/self.cfg.batch_size)+1)*self.cfg.one_cycle_epochs\n",
    "        continue_OneCycleLR = True\n",
    "        scheduler2 = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.3, div_factor=2, \n",
    "                                              max_lr=self.cfg.learning_rate*5, total_steps=total_steps_oneCycle,\n",
    "                                              )\n",
    "        scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=self.cfg.patience, factor=0.9, threshold=1e-5)\n",
    "    \n",
    "        #Print parameters\n",
    "        if hasattr(self.cfg, 'verbose'):\n",
    "            verbose = self.cfg.verbose\n",
    "        else:\n",
    "            verbose = 1\n",
    "        \n",
    "        #Train model\n",
    "        \n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            model.train()\n",
    "            avg_loss = 0.0\n",
    "            \n",
    "            if not self.moa_control_params is None:\n",
    "                if (self.moa_control_params['augment_data']) & (epoch>0):\n",
    "                    X_train_tmp = self._augment_data(X_train,\n",
    "                                                     self.moa_control_params['augment_var_list'],\n",
    "                                                     self.train_mask_non_control,\n",
    "                                                     epoch\n",
    "                                                    )\n",
    "                else:\n",
    "                    X_train_tmp = X_train.copy()\n",
    "                X_train_torch, y_train_torch = self._moa_add_control_data(X_train_tmp, y_train, control_size_to_add)\n",
    "            \n",
    "            #We create the train batch dataset here to be able to shuffle it\n",
    "            dataset = self._prepare_batch_data(X_train_torch,y_train_torch)\n",
    "            \n",
    "            #Train trhough batches\n",
    "            for X_batch, y_batch in dataset:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                # Forward pass: compute predicted y by passing x to the model.\n",
    "                y_pred = model(X_batch)\n",
    "                \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                avg_loss += loss.item() / (len(dataset))\n",
    "                \n",
    "                # Before the backward pass, use the optimizer object to zero all of the\n",
    "                # gradients for the variables it will update (which are the learnable\n",
    "                # weights of the model). This is because by default, gradients are\n",
    "                # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "                # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Backward pass: compute gradient of the loss with respect to model\n",
    "                # parameters\n",
    "                loss.backward()\n",
    "                \n",
    "                # Calling the step function on an Optimizer makes an update to its\n",
    "                # parameters\n",
    "                optimizer.step()\n",
    "                if (continue_OneCycleLR) and (self.cfg.number_one_cycle>0):\n",
    "                    scheduler2.step()\n",
    "\n",
    "            #OneCycle update\n",
    "            if (self.cfg.number_one_cycle==0):\n",
    "                continue_OneCycleLR = False\n",
    "            if (continue_OneCycleLR):\n",
    "                if (epoch+1)%(self.cfg.one_cycle_epochs*self.cfg.number_one_cycle)==0:\n",
    "                    print('Finish One Cycle')\n",
    "                    continue_OneCycleLR=False\n",
    "                    for g in optimizer.param_groups:\n",
    "                        g['lr'] = self.cfg.learning_rate\n",
    "                    \n",
    "                if ((epoch+1)%self.cfg.one_cycle_epochs==0) and (epoch+1)<(self.cfg.one_cycle_epochs*self.cfg.number_one_cycle):\n",
    "                    print('Reinitialize One Cycle')\n",
    "                    scheduler2 = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.3, div_factor=2, \n",
    "                                              max_lr=self.cfg.learning_rate*5, total_steps=total_steps_oneCycle,\n",
    "                                              )\n",
    "            \n",
    "            \n",
    "            #Evaluation\n",
    "            model.eval()\n",
    "            y_pred_val = self._predict_proba_model(model,X_val_torch,for_loss=True)\n",
    "            loss_val = loss_val_fn(y_pred_val, y_val_torch).item()\n",
    "            \n",
    "            check_early_stopping, is_new_best = self._should_stop(epoch,loss_val,model,self.cfg.early_stopping,self.cfg.min_delta,avg_loss,self.cfg.min_epochs,self.cfg.hard_patience,self.cfg.ratio_train_val)\n",
    "            if is_new_best:\n",
    "                text_new_best = 'New Best'\n",
    "            else:\n",
    "                text_new_best = ''\n",
    "            if (epoch+1)%verbose == 0:\n",
    "                print('Epoch {}/{} \\t train loss: {:.5f} \\t val loss: {:.5f} \\t time: {}s \\t lr: {:.6f} \\t {}'.format(epoch+1,self.cfg.epochs,avg_loss,loss_val, (datetime.datetime.now() - initial_time).seconds, optimizer.param_groups[-1]['lr'],text_new_best))\n",
    "                pass\n",
    "                \n",
    "            if not self.cfg.early_stopping is None:\n",
    "                if check_early_stopping:\n",
    "                    print(\"Early stopping, best epoch: {}\".format(self._best_epoch+1))\n",
    "                    model_to_return = self._load_best_model()\n",
    "                    return model_to_return\n",
    "                \n",
    "            if not self.moa_control_params is None: \n",
    "                if loss_val < self.moa_control_params['oof_loss_limit']:\n",
    "                    print('Model decrease below limit')\n",
    "                    return model\n",
    "\n",
    "            #Update lr when oneCycle is over\n",
    "            if not continue_OneCycleLR:\n",
    "                scheduler1.step(loss_val)\n",
    "        \n",
    "        model_to_return = self._load_best_model()\n",
    "        \n",
    "        return model_to_return\n",
    "    \n",
    "    def _load_best_model(self):\n",
    "        model_to_return = self.model_class(self.cfg)\n",
    "        model_to_return.to(self.device)\n",
    "        model_to_return.load_state_dict(torch.load('tmp_model_state_dict'))\n",
    "        return model_to_return\n",
    "    \n",
    "    \n",
    "    def save_model(self,name=''):\n",
    "        for i,model in enumerate(self.cv_models):\n",
    "            torch.save(model.state_dict(), name+'cv_'+str(i))\n",
    "        \n",
    "    def load_model(self,folds,name=''):\n",
    "        self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.cv_models = []\n",
    "        for i in range(folds):\n",
    "            model = self.model_class(self.cfg)\n",
    "            model.to(self.device)\n",
    "            model.load_state_dict(torch.load(name+'cv_'+str(i)))\n",
    "            self.cv_models.append(model)\n",
    "            \n",
    "    def load_oof_idx(self,oof_idx):\n",
    "        self._oof_idx = oof_idx\n",
    "    def load_X(self,X):\n",
    "        self._X = X\n",
    "            \n",
    "    def _should_stop(self,epoch,val_loss,model,patience,min_delta,train_loss,min_epochs=None,hard_patience=None,ratio_train_val=1.1):\n",
    "        if min_epochs is None:\n",
    "            min_epochs = 0\n",
    "        if hard_patience is None:\n",
    "            hard_patience = epoch + 1\n",
    "        if epoch==0:\n",
    "            self._best_val_loss = val_loss\n",
    "            #self._best_model = copy.deepcopy(model)\n",
    "            torch.save(model.state_dict(), 'tmp_model_state_dict')\n",
    "            self._best_epoch = epoch\n",
    "            self.epochs_since_best = 1\n",
    "        if val_loss < self._best_val_loss - min_delta:\n",
    "            self._best_val_loss = val_loss\n",
    "            #self._best_model = copy.deepcopy(model)\n",
    "            torch.save(model.state_dict(), 'tmp_model_state_dict')\n",
    "            self._best_epoch = epoch\n",
    "            self.epochs_since_best = 1\n",
    "            return False, True\n",
    "        else:\n",
    "            self.epochs_since_best += 1\n",
    "            if (epoch - self._best_epoch > patience) and (epoch > min_epochs) and ((val_loss>train_loss*ratio_train_val) or (epoch - self._best_epoch > hard_patience)):\n",
    "                return True, False\n",
    "            else:\n",
    "                return False, False\n",
    "\n",
    "    def _predict_proba_model(self, model, X, for_loss=False):\n",
    "        if isinstance(X,np.ndarray):\n",
    "            X = self._numpy_to_tensor(X)\n",
    "            \n",
    "        y_pred = []\n",
    "        \n",
    "        dataset = self._prepare_batch_data(X,inference=True)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_batch in dataset:\n",
    "                y = model(X_batch[0].to(self.device))\n",
    "                if for_loss:\n",
    "                    y_pred.append(y.detach().cpu())\n",
    "                else:\n",
    "                    if self.clamp is None:\n",
    "                        y_pred.append(y.sigmoid().detach().cpu().numpy())\n",
    "                    else:\n",
    "                        y_pred.append(y.sigmoid().clamp(self.clamp,1-self.clamp).detach().cpu().numpy())  \n",
    "        if for_loss:\n",
    "            y_pred = torch.cat(y_pred,dim=0)\n",
    "        else:\n",
    "            y_pred = np.concatenate(y_pred)\n",
    "        return y_pred\n",
    "        \n",
    "    def predict(self,X):\n",
    "        output_dim = self.cfg.target_cols\n",
    "        \n",
    "        y = np.zeros((X.shape[0],output_dim))\n",
    "        y = np.repeat(y[..., np.newaxis], len(self.cv_models), axis=-1)\n",
    "        for i,model in enumerate(self.cv_models):\n",
    "            y[...,i] = self._predict_proba_model(model,X)\n",
    "        y = y.mean(axis=-1)\n",
    "        return y\n",
    "    \n",
    "    def predict_oof(self):\n",
    "        y = (self._y.copy()).astype(float)\n",
    "        for (train_idx, val_idx), model in zip(self._oof_idx, self.cv_models):\n",
    "            y[val_idx,...] = self._predict_proba_model(model,self._X[val_idx])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nonscored_labels_pred(X_train,nn_model_non_scored,oof_idx):\n",
    "    X_train_nonscored_pred = np.append(X_train,np.zeros((X_train.shape[0],402)),axis=1)\n",
    "    for fold, (train_idx, val_idx) in enumerate(oof_idx):\n",
    "        X = X_train[val_idx,:]\n",
    "        nonscored_pred = nn_model_non_scored._predict_proba_model(nn_model_non_scored.cv_models[fold],X)\n",
    "        X_train_nonscored_pred[val_idx,-402:] = nonscored_pred\n",
    "    return X_train_nonscored_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_variation(var_list,mask):\n",
    "    var_list_new = []\n",
    "    for var in var_list:\n",
    "        var_list_new.append(var[mask].copy())\n",
    "    return var_list_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%debug\n",
    "if (run_script) and (calculate_new_prepared_data):\n",
    "    prepared_data = prepareData(path_data,validation_ratio=validation_ratio,folds=folds, cpu=cpu, normalization_type=normalization_type,\n",
    "                                g_removal_count=g_removal_count,exclude_c_from_kde=exclude_c_from_kde,exclude_g_from_kde=exclude_g_from_kde,add_c_stats=add_c_stats,\n",
    "                                add_kernels=add_kernels, use_diff_kde=use_diff_kde, use_train_test_for_norm=use_train_test_for_norm,\n",
    "                                perform_pca=perform_pca, pca_variance_threshold=0.95,pca_for_kde=pca_for_kde,pca_for_c=pca_for_c,\n",
    "                                use_log_for_kernel_diff=use_log_for_kernel_diff, inverse_kde=inverse_kde, ratio_inverse_kde=ratio_inverse_kde,\n",
    "                                granularity=granularity,max_dev=max_dev,normal_std_dev=normal_std_dev,additional=additional\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_script) and (calculate_new_prepared_data):\n",
    "    if dump_prepared_data:\n",
    "        pickle_dump(prepared_data,prepared_data_version)\n",
    "    if is_submission:\n",
    "        prepared_data.prepare_test_data(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_script) and (calculate_new_prepared_data==False):\n",
    "    if is_kaggle:\n",
    "        prepared_data = pickle_load('../input/moa-'+prepared_data_version+'/'+prepared_data_version)\n",
    "    else:\n",
    "        prepared_data = pickle_load(prepared_data_version)\n",
    "    if is_submission:\n",
    "        prepared_data.prepare_test_data(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%debug\n",
    "\n",
    "#Needs to be set after feature creation to avoid pickling issues\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if run_script and train_model:\n",
    "    nn_model_list = []\n",
    "    for i in range(n_seeds):\n",
    "        seed = SEED + i\n",
    "        print('\\n')\n",
    "        print('START WITH SEED: {}'.format(seed))\n",
    "\n",
    "        X_train,y_train,X_drugs,X_sig_id,X_holdout,y_holdout,mask_train = prepared_data.split_train_holdout(validation_ratio=validation_ratio,seed=seed,threshold=oof_threshold)\n",
    "        if oof_type=='multi':\n",
    "            oof_idx = prepared_data.create_cv(X_train,y_train,X_drugs,X_sig_id,folds=folds,seed=seed,threshold=oof_threshold)\n",
    "        else:\n",
    "            oof_idx = prepared_data.create_cv_kfold(X_train,X_drugs,X_sig_id,folds=folds,seed=seed)\n",
    "\n",
    "        X_test_control = prepared_data.X_test.values\n",
    "        mask = X_test_control[:,1]==1\n",
    "        X_test_control = X_test_control[mask][:,4:].copy()\n",
    "        X_test_control = X_test_control.astype(np.float32)\n",
    "\n",
    "        X = X_train.values\n",
    "        y = y_train.values\n",
    "\n",
    "        #Get treatment\n",
    "        mask_treatment = X[:,0]==0\n",
    "\n",
    "        #Keep only useful columns\n",
    "        X = X[:,3:].copy()\n",
    "\n",
    "        CFG.num_features=X.shape[1]\n",
    "        CFG.target_cols=y.shape[1]\n",
    "        \n",
    "        \n",
    "        #Data augmentation\n",
    "        var_list = None\n",
    "        if augment_data:\n",
    "            del var_list\n",
    "            gc.collect()\n",
    "            var_list = mask_variation(prepared_data.var_list,mask_train)\n",
    "\n",
    "\n",
    "        moa_control_params = {\n",
    "            'control_share': control_share_in_train,\n",
    "            'add_control_from_test': add_control_from_test,\n",
    "            'mask_treatment': mask_treatment,\n",
    "            'test_control_data': X_test_control,\n",
    "            'augment_data': augment_data,\n",
    "            'augment_var_list': var_list,\n",
    "            'label_smoothing': label_smoothing,\n",
    "            'oof_loss_limit': oof_loss_limit,\n",
    "            'fold_type':fold_type\n",
    "        }\n",
    "\n",
    "        nn_model = NNWrapper(TabularNN,CFG)\n",
    "        nn_model.fit(X,y,folds=folds,evaluate=False,oof_idx=oof_idx,moa_control_params=moa_control_params,seed=seed)\n",
    "\n",
    "        dict_model = {\n",
    "            'nn_model':nn_model,\n",
    "            'X_train':X_train,\n",
    "            'y_train':y_train,\n",
    "            'X_holdout':X_holdout,\n",
    "            'y_holdout':y_holdout\n",
    "        }\n",
    "        nn_model_list.append(dict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    for i,model_dict in enumerate(nn_model_list):\n",
    "        model_dict['nn_model'].save_model(name='seed_'+str(i)+'_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class post_process():\n",
    "    def __init__(self,X,y):\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "        \n",
    "    def control_to_zero(self,y=None):\n",
    "        if y is None:\n",
    "            y = self.y.copy()\n",
    "        control_idx = self.X[:,0]==1\n",
    "        y[control_idx,:] = 0\n",
    "        return y\n",
    "    \n",
    "    def label_smoothing(self,ls_floor,ls_ceil,with_control=True):\n",
    "        y = self.y.copy()\n",
    "        y = y.clip(ls_floor,1-ls_ceil)\n",
    "        if with_control:\n",
    "            y = self.control_to_zero(y)\n",
    "        return y\n",
    "    \n",
    "    def exclusivity(self,exclusivity_tuples_idx,ls_floor,ls_ceil,activation_threshold=0.3,floor=1e-5):\n",
    "        y = self.y.copy()\n",
    "        y = self.label_smoothing(ls_floor,ls_ceil)\n",
    "        \n",
    "        for tup in exclusivity_tuples_idx:\n",
    "            y_tmp = y[:,tup]\n",
    "\n",
    "            max_val = np.amax(y_tmp,axis=1)\n",
    "            max_mask = max_val>=activation_threshold\n",
    "            max_mask = np.repeat(np.array([max_mask]),y_tmp.shape[1],axis=0).transpose()\n",
    "\n",
    "            max_idx = np.argmax(y_tmp,axis=1)\n",
    "            max_idx = np.repeat(np.array([max_idx]),y_tmp.shape[1],axis=0).transpose()\n",
    "            y_idx = np.repeat(np.array([np.arange(0,len(tup))]),y_tmp.shape[0],axis=0)\n",
    "\n",
    "            y_mask = (y_idx!=max_idx) & (max_mask)\n",
    "            y_global_mask = y==-1\n",
    "            y_global_mask[:,tup] = y_mask\n",
    "            y[y_global_mask] = np.clip(y[y_global_mask],0,floor)\n",
    "        return y\n",
    "    \n",
    "def get_available_exclusivity_tuples(prepared_data,\n",
    "                                     model_nonscored=model_nonscored,\n",
    "                                     exclusivity_tuples=exclusivity_tuples\n",
    "                                     ):\n",
    "    available_cols = prepared_data.y_train.columns.tolist()\n",
    "    if model_nonscored:\n",
    "        available_cols = available_cols + prepared_data.X_train_nonscored.columns.tolist()\n",
    "\n",
    "    exclusivity_tuples_idx = []\n",
    "    for tup in exclusivity_tuples:\n",
    "        if all([x in available_cols for x in tup]):\n",
    "            idx = []\n",
    "            for col in tup:\n",
    "                idx.append(available_cols.index(col))\n",
    "            exclusivity_tuples_idx.append(idx)\n",
    "\n",
    "    return exclusivity_tuples_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn_model_list(nn_model_list,ls_floor=1e-5,ls_ceil=1e-5,activation_threshold=0.3,floor=1e-5):\n",
    "    results = []\n",
    "    for model_dict in nn_model_list:\n",
    "        results.append(\n",
    "            evaluate_nn_model(\n",
    "                model_dict['X_train'],\n",
    "                model_dict['y_train'],\n",
    "                model_dict['X_holdout'],\n",
    "                model_dict['y_holdout'],\n",
    "                model_dict['nn_model'],\n",
    "                ls_floor,\n",
    "                ls_ceil,\n",
    "                activation_threshold,\n",
    "                floor\n",
    "            )\n",
    "        )\n",
    "    results = np.array(results)\n",
    "    results_mean = results.mean(axis=0)\n",
    "    print(\"oof: {:.6f} \\t oof post: {:.6f} \\t oof ls: {:.6f} \\t oof excl: {:.6f} \\t holdout: {:.6f} \\t holdout post: {:.6f} \\t holdout ls: {:.6f} \\t holdout excl: {:.6f}\".format(\n",
    "        results_mean[0],results_mean[1],results_mean[2],results_mean[3],\n",
    "        results_mean[4],results_mean[5],results_mean[6],results_mean[7]\n",
    "    ))\n",
    "\n",
    "def evaluate_nn_model(X_train,y_train,X_holdout,y_holdout,nn_model,ls_floor=1e-5,ls_ceil=1e-5,activation_threshold=0.3,floor=1e-5):\n",
    "    exclusivity_tuples_idx = get_available_exclusivity_tuples(prepared_data)\n",
    "    #OOF\n",
    "    y_oof_pred = (y_train.values.copy()).astype(float)\n",
    "    X_oof = X_train.values \n",
    "    y_oof_pred_all = y_oof_pred\n",
    "    X_oof_all = X_oof\n",
    "    \n",
    "    for (train_idx, val_idx), model in zip(nn_model._oof_idx, nn_model.cv_models):\n",
    "        y_oof_pred[val_idx,...] = nn_model._predict_proba_model(model,nn_model._X[val_idx])\n",
    "    \n",
    "    postProcess_oof = post_process(X_oof_all,y_oof_pred_all)\n",
    "    y_oof_post_processed = postProcess_oof.control_to_zero()\n",
    "    y_oof_post_processed_ls = postProcess_oof.label_smoothing(ls_floor,ls_ceil)\n",
    "    y_oof_post_processed_exclusivity = postProcess_oof.exclusivity(exclusivity_tuples_idx,ls_floor,ls_ceil,activation_threshold,floor)\n",
    "    \n",
    "    oof_loss = metrics.log_loss(y_train.values.reshape(-1,1),y_oof_pred_all.reshape(-1,1))\n",
    "    oof_loss_post_processed = metrics.log_loss(y_train.values.reshape(-1,1),y_oof_post_processed.reshape(-1,1))\n",
    "    oof_loss_post_processed_ls = metrics.log_loss(y_train.values.reshape(-1,1),y_oof_post_processed_ls.reshape(-1,1))\n",
    "    oof_loss_post_processed_exclusivity = metrics.log_loss(y_train.values.reshape(-1,1),y_oof_post_processed_exclusivity.reshape(-1,1))\n",
    "    \n",
    "    #Holdout\n",
    "    y_holdout_pred = nn_model.predict(X_holdout.values[:,3:])\n",
    "    \n",
    "    postProcess_holdout = post_process(X_holdout.values,y_holdout_pred)\n",
    "    \n",
    "    y_holdout_pred_post_processed = postProcess_holdout.control_to_zero()\n",
    "    y_holdout_pred_post_processed_ls = postProcess_holdout.label_smoothing(ls_floor,ls_ceil)\n",
    "    y_holdout_pred_post_processed_exclusivity = postProcess_holdout.exclusivity(exclusivity_tuples_idx,ls_floor,ls_ceil,activation_threshold,floor)\n",
    "    \n",
    "    holdout_loss = metrics.log_loss(y_holdout.values.reshape(-1,1),y_holdout_pred.reshape(-1,1))\n",
    "    holdout_loss_post_processed = metrics.log_loss(y_holdout.values.reshape(-1,1),y_holdout_pred_post_processed.reshape(-1,1))\n",
    "    holdout_loss_post_processed_ls = metrics.log_loss(y_holdout.values.reshape(-1,1),y_holdout_pred_post_processed_ls.reshape(-1,1))\n",
    "    holdout_loss_post_processed_exclusivity = metrics.log_loss(y_holdout.values.reshape(-1,1),y_holdout_pred_post_processed_exclusivity.reshape(-1,1))\n",
    "    \n",
    "#     print(\"oof: {:.6f} \\t oof post: {:.6f} \\t oof ls: {:.6f} \\t oof excl: {:.6f} \\t holdout: {:.6f} \\t holdout post: {:.6f} \\t holdout ls: {:.6f} \\t holdout excl: {:.6f}\".format(\n",
    "#         oof_loss,oof_loss_post_processed,oof_loss_post_processed_ls,oof_loss_post_processed_exclusivity,\n",
    "#         holdout_loss,holdout_loss_post_processed,holdout_loss_post_processed_ls,holdout_loss_post_processed_exclusivity\n",
    "#     ))\n",
    "    return [oof_loss,oof_loss_post_processed,oof_loss_post_processed_ls,oof_loss_post_processed_exclusivity,\n",
    "        holdout_loss,holdout_loss_post_processed,holdout_loss_post_processed_ls,holdout_loss_post_processed_exclusivity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(prepared_data,nn_model_list,ls_floor=1e-5,ls_ceil=1e-5,cfg=CFG,path_data=path_data):\n",
    "    preds_list = []\n",
    "    for model_dict in nn_model_list:\n",
    "        preds_list.append(\n",
    "            model_dict['nn_model'].predict(prepared_data.X_test.values[:,4:].astype(np.float32))\n",
    "        )\n",
    "    preds = np.array(preds_list)\n",
    "    preds = preds.mean(axis=0)\n",
    "    \n",
    "    postProcess = post_process(prepared_data.X_test.iloc[:,1:].values,preds)\n",
    "    preds = postProcess.label_smoothing(ls_floor,ls_ceil)\n",
    "    submission = pd.read_csv(path_data+'sample_submission.csv')\n",
    "    submission.iloc[:,1:] = preds\n",
    "    submission.to_csv('submission.csv',index=False)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model==False:\n",
    "    nn_model_list = []\n",
    "    for i in range(n_seeds):\n",
    "        seed = 42 + i\n",
    "        X_train,y_train,X_drugs,X_sig_id,X_holdout,y_holdout,mask_train = prepared_data.split_train_holdout(validation_ratio=validation_ratio,seed=seed,threshold=oof_threshold)\n",
    "        if oof_type=='multi':\n",
    "            oof_idx = prepared_data.create_cv(X_train,y_train,X_drugs,X_sig_id,folds=folds,seed=seed,threshold=oof_threshold)\n",
    "        else:\n",
    "            oof_idx = prepared_data.create_cv_kfold(X_train,X_drugs,X_sig_id,folds=folds,seed=seed)\n",
    "        \n",
    "        CFG.num_features=X_train.values[:,3:].shape[1]\n",
    "        CFG.target_cols=y_train.shape[1]\n",
    "        \n",
    "        nn_model = NNWrapper(TabularNN,CFG)\n",
    "        nn_model.load_model(folds,path_model+'seed_'+str(i)+'_')\n",
    "        nn_model.load_oof_idx(oof_idx)\n",
    "        nn_model.load_X(X_train.values[:,3:])\n",
    "        \n",
    "        dict_model = {\n",
    "            'nn_model':nn_model,\n",
    "            'X_train':X_train,\n",
    "            'y_train':y_train,\n",
    "            'X_holdout':X_holdout,\n",
    "            'y_holdout':y_holdout\n",
    "        }\n",
    "        \n",
    "        nn_model_list.append(dict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_script:\n",
    "    if (is_submission):\n",
    "        make_submission(prepared_data,nn_model_list)\n",
    "    if (not is_submission) and (not is_train_without_holdout):\n",
    "        evaluate_nn_model_list(nn_model_list,ls_floor=1e-5,ls_ceil=1e-5,activation_threshold=0.3,floor=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
